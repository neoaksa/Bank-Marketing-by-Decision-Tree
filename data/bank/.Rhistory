setwd("~/taoj@mail.gvsu.edu/gvsu/course/CIS678/Bank Marketing by Decision Tree/data/bank")
bank <- read.table("bank-full.csv", sep=";", header=T)
library(data.tree)
# check the pure, the class should be last column
IsPure <- function(data) {
length(unique(data[,ncol(data)])) == 1
}
# check the entropoy``
Entropy <- function( vls ) {
res <- vls/sum(vls) * log2(vls/sum(vls))
res[vls == 0] <- 0
-sum(res)
}
# cal gain
InformationGain <- function( tble ) {
tble <- as.data.frame.matrix(tble)
entropyBefore <- Entropy(colSums(tble))
s <- rowSums(tble)
entropyAfter <- sum (s / sum(s) * apply(tble, MARGIN = 1, FUN = Entropy ))
informationGain <- entropyBefore - entropyAfter
return (informationGain)
}
# retrun p of majority  in a given vector
predict.outcome <- function(outcome.vector){
# Predict class based on outcome.vector
# Determine possible outcomes and percentage occurrence
outcomes <- unique(outcome.vector)
outcome.counts <- rep(0, length(outcomes))
names(outcome.counts) <- as.character(outcomes)
# count num for each element in vecotr
for(item in names(outcome.counts)){
outcome.counts[item] <- sum(outcome.vector == item)
}
# calc P
outcome.counts <- outcome.counts / sum(outcome.counts)
# return prediction based on max outcome counts
max.index <- which(outcome.counts == max(outcome.counts))
return(list('predict' = names(outcome.counts)[max.index][1],
'margin'  = max(outcome.counts)))
}
# find the initial node
rootNode = function(data){
ig <- sapply(colnames(data)[-ncol(data)],
function(x) InformationGain(
table(data[,x], data[,ncol(data)])
)
)
feature <- names(ig)[ig == max(ig)][1]
return(feature)
}
# train model
TrainID3 <- function(node, data, thredhold, purity) {
node$obsCount <- nrow(data)
# set the purity to nodes
node$purity <- predict.outcome(data[,ncol(data)])$margin
#if the data-set is pure, then
if (IsPure(data)) {
#construct a leaf having the name of the pure feature
child <- node$AddChild(unique(data[,ncol(data)]))
child$purity <- 1
node$feature <- tail(names(data), 1)
child$obsCount <- nrow(data)
child$feature <- ''
}
# no attribute can be used for spliting or less then threshold num
else if(ncol(data)==1 | nrow(data)<thredhold | node$purity>purity)
{
predict <- predict.outcome(data[,ncol(data)])
child <- node$AddChild(predict$predict)
child$purity <- predict$margin
node$feature <- tail(names(data), 1)
child$obsCount <- nrow(data)
child$feature <- ''
}
else {
#choose the feature with the highest information gain
ig <- sapply(colnames(data)[-ncol(data)],
function(x) InformationGain(
table(data[,x], data[,ncol(data)])
)
)
feature <- names(ig)[ig == max(ig)][1]
node$feature <- feature
#take the subset of the data-set having that feature value
childObs <- split(data[,!(names(data) %in% feature)], data[,feature], drop = TRUE)
for(i in 1:length(childObs)) {
#construct a child having the name of that feature value
# child <- node$AddChild(paste(feature,":",names(childObs)[i]))
child <- node$AddChild(names(childObs)[i])
#if(class(childObs[[i]])=="factor"){childObs[[i]] <-as.data.frame(childObs[[i]])}
childObs[[i]] <-as.data.frame(childObs[[i]])
#call the algorithm recursively on the child and the subset
TrainID3(child, childObs[[i]], thredhold,purity)
}
}
}
# prediction
Predict <- function(tree, features) {
if (tree$children[[1]]$isLeaf) {
return (tree$children[[1]]$name)}
child <- tree$children[[as.character(features[tree$feature][[1]])]]
if(is.null(child)){
return ("unknow")}
return ( Predict(child, features))
}
# automatic bin
binconti <- function(df, conti.name, class.name){
subdf <- df[,c(conti.name,class.name)]
# sort by continus numbers ascedently
subdf <- subdf[order(subdf[,conti.name],subdf[,class.name]),]
# find the split point
temp <- subdf[1, class.name] # save previous point
enthropy.orgi <- Entropy(table(subdf[,class.name]))
rownum <- nrow(subdf)
# record gain and split point
gain <- NULL
splitpoint <- NULL
for(i in 2: rownum){
if(temp != subdf[i, class.name]){
# calculate inforgain
enthropy1 <- Entropy(table(subdf[1:i-1,class.name]))
enthropy2 <- Entropy(table(subdf[i:rownum,class.name]))
gain <- append(gain,enthropy.orgi - ((i-1)/rownum)*enthropy1 - ((rownum-i+1)/rownum)*enthropy2)
splitpoint <- append(splitpoint, subdf[i,conti.name])
temp <- subdf[i,class.name] # change to new class
}
}
point <- cbind(splitpoint, gain)
return (point)
}
p<-binconti(bank, "age","y")
plot(p)
bank$age = cut(bank$age, breaks=c(-Inf, 61, Inf), labels=c("<61",">=61"))
summary(bank$age)
p<-binconti(bank, "balance","y")
plot(p)
partition.table = table(bank$balance,bank$y)
row.sums = as.vector(rowSums(partition.table))
prob = partition.table / row.sums
mosaicplot(partition.table , shade = T, xlab = "balance", ylab = "y", main = "Mosaic Plot")
table(bank$pdays)  # too many nosie, skip
p<-binconti(bank, "pdays","y")
plot(p)
pairs(~y+month+contact+duration+pdays, data=bank)
pairs(~y+day+campaign+previous, data=bank)
p<-binconti(bank, "balance","y")
plot(p)
