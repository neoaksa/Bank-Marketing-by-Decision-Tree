resample <- function(x, ...) x[sample.int(length(x), ...)]
resample(x[x >  9]) # length 1
data.frame(replicate(10,sample(0:100,1000,rep=TRUE)))
test.data<--data.frame(replicate(10,sample(0:100,1000,rep=TRUE)))
head(test.data)
summary(test.data)
sum.test<-summary(test.data$X1)
sum.test
test.data<--data.frame(replicate(10,sample(0:100,1000,rep=TRUE)))
sum.test<-summary(test.data$X1)
count<-table(test.data$X1,test.data$X2,dnn=c("column1","column2"))
count
count
hist(test.data$X1)
library(ggplot2)
install.packages("ggplot2")
95/2
install.packages("neuralnet")
c(3:5, 11:8, 8 + 0:5)
x <- c(3:5, 11:8, 8 + 0:5)
(ux <- unique(x))
Var1 <- runif(50, 0, 100)
sqrt.data <- data.frame(Var1, Sqrt=sqrt(Var1))
print(net.sqrt <- neuralnet(Sqrt~Var1,  sqrt.data, hidden=10,
threshold=0.01))
compute(net.sqrt, (1:10)^2)$net.result
Var1 <- runif(50, 0, 100)
sqrt.data <- data.frame(Var1, Sqrt=sqrt(Var1))
print(net.sqrt <- neuralnet(Sqrt~Var1,  sqrt.data, hidden=10,
threshold=0.01))
compute(net.sqrt, (1:10)^2)$net.result
library(neuralnet)
Var1 <- runif(50, 0, 100)
sqrt.data <- data.frame(Var1, Sqrt=sqrt(Var1))
print(net.sqrt <- neuralnet(Sqrt~Var1,  sqrt.data, hidden=10,
threshold=0.01))
compute(net.sqrt, (1:10)^2)$net.result
View(sqrt.data)
View(sqrt.data)
install.packages("kohonen")
install.packages("arules")
## example 1: creating transactions form a list
a_list <- list(
c("a","b","c"),
c("a","b"),
c("a","b","d"),
c("c","e"),
c("a","b","d","e")
)
## example 1: creating transactions form a list
a_list <- list(
c("a","b","c"),
c("a","b"),
c("a","b","d"),
c("c","e"),
c("a","b","d","e")
)
names(a_list) <- paste("Tr",c(1:5), sep = "")
a_list
trans1 <- as(a_list, "transactions")
library("arules")
## coerce into transactions
trans1 <- as(a_list, "transactions")
trans1
ds_iris<-data("iris")
ds_iris<-data("iris")
library(ggplot2)
ggplot(ds_iris,aes(species,Sepal.Length)+
geom_point()
ggplot(ds_iris,aes(species,Sepal.Length)
ggplot(ds_iris,aes(species,Sepal.Length)+
geom_point()
ggplot(ds_iris,aes(species,Sepal.Length))+
geom_point()
ggplot(ds_iris,aes(species,Sepal.Length))
ggplot(iris,aes(species,Sepal.Length))+
geom_point()
ggplot(iris,aes(species,Sepal.Length))
ggplot(iris,aes(Species,Sepal.Length))
ggplot(iris,aes(Species,Sepal.Length))+
geom_point()
ggplot(ds_iris,aes(Species,Sepal.Length))+
geom_point()
ggplot(iris,aes(Species,Sepal.Length))+
geom_point()
who = list(name="Joe", age=45, married=T)
print(who)
print(who$name)
print(who[[1]])
print(who$age)
print(who[[2]])
print(who$married)
print(who[[3]])
names(who)
x1 = seq(1,10,length=25)
x2 = runif(25,3,7)
## and you want to fit the model:
y = 4 + 2*x1 + 7*x2 + rnorm(25,0,1)
## First create a data frame
mydata = data.frame(y=y,x1=x1,x2=x2)
lm.out = lm(y ~ x1 + x2, data = mydata)
names(lm.out)
summary(lm.out)
plot(lm.out,ask=F)  ## Plots the fitted curve
par(mfrow=c(2,2))
plot(lm.out,ask=F)
plot(lm.out,ask=F)
par(mfrow=c(1,1))
plot(lm.out,ask=F)
plot(lm.out,ask=F)
par(mfrow=c(2,2))   # Set graphics parameter
plot(lm.out,ask=F)
m <- matrix(1:4, 2)
m
prop.table(m, 1)
m <- matrix(1:4, 2)
m
prop.table(m)
prop.table(table(m)
m <- matrix(1:4)
m
prop.table(table(m))
m <- matrix(1:4, 2)
m
prop.table(m, 1)
consensus(matali, method = c( "majority", "threshold", "IUPAC", "profile")
consensus(matali, method = c( "majority", "threshold", "IUPAC", "profile"),
threshold = 0.60, warn.non.IUPAC = FALSE, type = c("DNA", "RNA"))
install.packages("entropy")
# load entropy library
library("entropy")
x1 = runif(10000)
x1 = runif(10000)
x1
hist(x1, xlim=c(0,1), freq=FALSE)
y1 = discretize(x1, numBins=10, r=c(0,1))
y1
setwd("~/Google Drive/gvsu/course/CIS678/Bank Marketing by Decision Tree/data/bank")
library(data.tree)
setwd("~/Google Drive/gvsu/course/CIS678/Bank Marketing by Decision Tree/data/bank")
# check the pure, the class should be last column
IsPure <- function(data) {
length(unique(data[,ncol(data)])) == 1
}
# check the entropoy``
Entropy <- function( vls ) {
res <- vls/sum(vls) * log2(vls/sum(vls))
res[vls == 0] <- 0
-sum(res)
}
# cal gain
InformationGain <- function( tble ) {
tble <- as.data.frame.matrix(tble)
entropyBefore <- Entropy(colSums(tble))
s <- rowSums(tble)
entropyAfter <- sum (s / sum(s) * apply(tble, MARGIN = 1, FUN = Entropy ))
informationGain <- entropyBefore - entropyAfter
return (informationGain)
}
# retrun p of majority  in a given vector
predict.outcome <- function(outcome.vector){
# Predict class based on outcome.vector
# Determine possible outcomes and percentage occurrence
outcomes <- unique(outcome.vector)
outcome.counts <- rep(0, length(outcomes))
names(outcome.counts) <- as.character(outcomes)
# count num for each element in vecotr
for(item in names(outcome.counts)){
outcome.counts[item] <- sum(outcome.vector == item)
}
# calc P
outcome.counts <- outcome.counts / sum(outcome.counts)
# return prediction based on max outcome counts
max.index <- which(outcome.counts == max(outcome.counts))
return(list('predict' = names(outcome.counts)[max.index][1],
'margin'  = max(outcome.counts)))
}
# train model
TrainID3 <- function(node, data) {
node$obsCount <- nrow(data)
#if the data-set is pure, then
if (IsPure(data)) {
#construct a leaf having the name of the pure feature
child <- node$AddChild(unique(data[,ncol(data)]))
node$feature <- tail(names(data), 1)
child$obsCount <- nrow(data)
child$feature <- ''
}
# no attributie can be used for spliting
else if(ncol(data)==1)
{
predict <- predict.outcome(data[,ncol(data)])
child <- node$AddChild(predict$predict)
node$feature <- tail(names(data), 1)
child$obsCount <- nrow(data)
child$feature <- ''
}
else {
#chose the feature with the highest information gain
ig <- sapply(colnames(data)[-ncol(data)],
function(x) InformationGain(
table(data[,x], data[,ncol(data)])
)
)
feature <- names(ig)[ig == max(ig)][1]
node$feature <- feature
#take the subset of the data-set having that feature value
childObs <- split(data[,!(names(data) %in% feature)], data[,feature], drop = TRUE)
for(i in 1:length(childObs)) {
#construct a child having the name of that feature value
child <- node$AddChild(names(childObs)[i])
if(class(childObs[[i]])=="factor"){childObs[[i]] <-as.data.frame(childObs[[i]])}
#call the algorithm recursively on the child and the subset
TrainID3(child, childObs[[i]])
}
}
}
# prediction
Predict <- function(tree, features) {
if (tree$children[[1]]$isLeaf) return (tree$children[[1]]$name)
child <- tree$children[[features[[tree$feature]]]]
return ( Predict(child, features))
}
# start to prepare date
bank <- read.table("bank-full.csv", sep=";", header=T)
# bin for age
# banktemp <-bank
# bank <-banktemp
# bank$age <- cut(bank$age, breaks=c(-Inf, 20,30, 40,50,60,Inf),
#                 labels=c("~20","21~30","31~40","41~50","51~60","6~"))
bank$age <- cut(bank$age, breaks=c(-Inf, 30,55,Inf),
labels=c("~30","31~60","55~"))
partition.table = table(bank$age,bank$y)
row.sums = as.vector(rowSums(partition.table))
prob = partition.table / row.sums
mosaicplot(partition.table , shade = T, xlab = "age", ylab = "y", main = "Mosaic Plot")
# bin for balance
# bank <-banktemp
summary(bank$balance)
# bank$balance <- cut(bank$balance, breaks=c(-Inf, 72,1362,1428,Inf),
#                 labels=c("1","2","3","4"))
bank$balance <- cut(bank$balance, breaks=c(-Inf, 72,Inf),
labels=c("<72",">=72"))
partition.table = table(bank$balance,bank$y)
row.sums = as.vector(rowSums(partition.table))
prob = partition.table / row.sums
mosaicplot(partition.table , shade = T, xlab = "balance", ylab = "y", main = "Mosaic Plot")
# bin for pday
table(bank$pdays)  # too many nosie, skip
# start to build tree
# delete unrelated attribute
MyData = subset(bank,select = -c(month,contact,duration,pdays,day,campaign,previous))
# split to training and test data
MyData <- MyData[sample(1:nrow(MyData)),]
MyTest <- MyData[40001:45211,]
MyTrain <- MyData[1:40000,]
library(ROSE)
# oversampling traning data
MyTrain <- ovun.sample(y ~ ., data = MyTrain, method = "both",N = 40000, p=0.5, seed=1)$data
# MyTrain <- ovun.sample(y ~ ., data = MyTrain, method = "under",N = 20000, seed =1)$data
# MyTrain <- ovun.sample(y ~ ., data = MyTrain, method = "both",N = 30000,  p=0.5,seed=1)$data
# cut train and validation data
MyTrain <- MyTrain[sample(1:nrow(MyData)),]
MyValidation <- MyTrain[35000:40000,]
MyValidation <- MyValidation[MyValidation$y %in% c("yes","no"),]
MyTrain <-MyTrain[1:35000,]
MyTrain <- MyTrain[MyTrain$y %in% c("yes","no"),]
# train the tree
tree <- Node$new("bank")
TrainID3(tree, MyTrain)
# predictions <- apply(MyValidation[,1:9],MARGIN = 1,Predict)
# Predict(tree,MyValidation[1,1:9])
print(tree, "feature", "obsCount")
Prune(tree, function(x) x$obsCount> 3000)
plot(tree)
setwd("~/Google Drive/gvsu/course/CIS678/Bank Marketing by Decision Tree/data/bank")
# this downloads and unzips the dataset
temp <- tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip",temp, mode="wb")
unzip(temp, "bank-full.csv")
unlink(temp)
bank <- read.table("bank-full.csv", sep=";", header=T)
library(data.tree)
# check the pure, the class should be last column
IsPure <- function(data) {
length(unique(data[,ncol(data)])) == 1
}
# check the entropoy``
Entropy <- function( vls ) {
res <- vls/sum(vls) * log2(vls/sum(vls))
res[vls == 0] <- 0
-sum(res)
}
# cal gain
InformationGain <- function( tble ) {
tble <- as.data.frame.matrix(tble)
entropyBefore <- Entropy(colSums(tble))
s <- rowSums(tble)
entropyAfter <- sum (s / sum(s) * apply(tble, MARGIN = 1, FUN = Entropy ))
informationGain <- entropyBefore - entropyAfter
return (informationGain)
}
# retrun p of majority  in a given vector
predict.outcome <- function(outcome.vector){
# Predict class based on outcome.vector
# Determine possible outcomes and percentage occurrence
outcomes <- unique(outcome.vector)
outcome.counts <- rep(0, length(outcomes))
names(outcome.counts) <- as.character(outcomes)
# count num for each element in vecotr
for(item in names(outcome.counts)){
outcome.counts[item] <- sum(outcome.vector == item)
}
# calc P
outcome.counts <- outcome.counts / sum(outcome.counts)
# return prediction based on max outcome counts
max.index <- which(outcome.counts == max(outcome.counts))
return(list('predict' = names(outcome.counts)[max.index][1],
'margin'  = max(outcome.counts)))
}
# train model
TrainID3 <- function(node, data) {
node$obsCount <- nrow(data)
#if the data-set is pure, then
if (IsPure(data)) {
#construct a leaf having the name of the pure feature
child <- node$AddChild(unique(data[,ncol(data)]))
node$feature <- tail(names(data), 1)
child$obsCount <- nrow(data)
child$feature <- ''
}
# no attribute can be used for spliting
else if(ncol(data)==1)
{
predict <- predict.outcome(data[,ncol(data)])
child <- node$AddChild(predict$predict)
node$feature <- tail(names(data), 1)
child$obsCount <- nrow(data)
child$feature <- ''
}
else {
#choose the feature with the highest information gain
ig <- sapply(colnames(data)[-ncol(data)],
function(x) InformationGain(
table(data[,x], data[,ncol(data)])
)
)
feature <- names(ig)[ig == max(ig)][1]
node$feature <- feature
#take the subset of the data-set having that feature value
childObs <- split(data[,!(names(data) %in% feature)], data[,feature], drop = TRUE)
for(i in 1:length(childObs)) {
#construct a child having the name of that feature value
child <- node$AddChild(names(childObs)[i])
if(class(childObs[[i]])=="factor"){childObs[[i]] <-as.data.frame(childObs[[i]])}
#call the algorithm recursively on the child and the subset
TrainID3(child, childObs[[i]])
}
}
}
# prediction
Predict <- function(tree, features) {
if (tree$children[[1]]$isLeaf) return (tree$children[[1]]$name)
child <- tree$children[[features[[tree$feature]]]]
return ( Predict(child, features))
}
# start to prepare date
#bank <- read.table("bank-full.csv", sep=";", header=T)
# bin for age
# banktemp <-bank
# bank <-banktemp
# bank$age <- cut(bank$age, breaks=c(-Inf, 20,30, 40,50,60,Inf),
#                 labels=c("~20","21~30","31~40","41~50","51~60","6~"))
bank$age <- cut(bank$age, breaks=c(-Inf, 30,55,Inf),
labels=c("~30","31~60","55~"))
partition.table = table(bank$age,bank$y)
row.sums = as.vector(rowSums(partition.table))
prob = partition.table / row.sums
mosaicplot(partition.table , shade = T, xlab = "age", ylab = "y", main = "Mosaic Plot")
# bin for balance
# bank <-banktemp
summary(bank$balance)
# bank$balance <- cut(bank$balance, breaks=c(-Inf, 72,1362,1428,Inf),
#                 labels=c("1","2","3","4"))
bank$balance <- cut(bank$balance, breaks=c(-Inf, 72,Inf),
labels=c("<72",">=72"))
partition.table = table(bank$balance,bank$y)
row.sums = as.vector(rowSums(partition.table))
prob = partition.table / row.sums
mosaicplot(partition.table , shade = T, xlab = "balance", ylab = "y", main = "Mosaic Plot")
# bin for pday
table(bank$pdays)  # too many nosie, skip
# start to build tree
# delete unrelated attribute
MyData = subset(bank,select = -c(month,contact,duration,pdays,day,campaign,previous))
# split to training and test data
MyData <- MyData[sample(1:nrow(MyData)),]
MyTest <- MyData[40001:45211,]
MyTrain <- MyData[1:40000,]
library(ROSE)
# oversampling traning data
MyTrain <- ovun.sample(y ~ ., data = MyTrain, method = "both",N = 40000, p=0.5, seed=1)$data
# MyTrain <- ovun.sample(y ~ ., data = MyTrain, method = "under",N = 20000, seed =1)$data
# MyTrain <- ovun.sample(y ~ ., data = MyTrain, method = "both",N = 30000,  p=0.5,seed=1)$data
# cut train and validation data
MyTrain <- MyTrain[sample(1:nrow(MyData)),]
MyValidation <- MyTrain[35000:40000,]
MyValidation <- MyValidation[MyValidation$y %in% c("yes","no"),]
MyTrain <-MyTrain[1:35000,]
MyTrain <- MyTrain[MyTrain$y %in% c("yes","no"),]
# class example for checking against a known calculation
wind = c('strong','weak','strong','strong','strong','weak','weak','strong','strong','strong','weak','weak','strong','weak')
water = c('warm','warm','warm','moderate','cold','cold','cold','moderate','cold','moderate','moderate','moderate','warm','moderate')
air = c('warm','warm','warm','warm','cool','cool','cool','warm','cool','cool','cool','warm','cool','warm')
forecast = c('sunny','sunny','cloudy','rainy','rainy','rainy','sunny','sunny','sunny','rainy','sunny','sunny','sunny','rainy')
oracle = c('yes','no','yes','yes','no','no','no','yes','yes','no','yes','yes','yes','no')
example = data.frame(wind, water, air, forecast, oracle)
ig <- sapply(colnames(example)[-ncol(example)],
function(x) InformationGain(
table(example[,x], example[,ncol(example)])
)
)
feature <- names(ig)[ig == max(ig)][1]
startNode <- feature
extree <- Node$new(startNode)
TrainID3(extree, example)
plot(extree)
# train the tree
tree <- Node$new("bank")
TrainID3(tree, MyTrain)
predictions <- apply(MyValidation[,1:9],MARGIN = 1,Predict)
Predict(tree,MyValidation[1,1:9])
print(tree, "feature", "obsCount")
Prune(tree, function(x) x$obsCount> 3000)
plot(tree)
ig <- sapply(colnames(MyTrain)[-ncol(MyTrain)],
function(x) InformationGain(
table(MyTrain[,x], example[,ncol(MyTrain)])
)
)
feature <- names(ig)[ig == max(ig)][1]
startNode <- feature
extree <- Node$new(startNode)
TrainID3(extree, MyTrain)
plot(extree)
ig <- sapply(colnames(MyTrain)[-ncol(MyTrain)],
function(x) InformationGain(
table(MyTrain[,x], MyTrain[,ncol(MyTrain)])
)
)
feature <- names(ig)[ig == max(ig)][1]
startNode <- feature
extree <- Node$new(startNode)
TrainID3(extree, MyTrain)
ig <- sapply(colnames(MyTrain)[-ncol(MyTrain)],
function(x) InformationGain(
table(MyTrain[,x], MyTrain[,ncol(MyTrain)])
)
)
feature <- names(ig)[ig == max(ig)][1]
startNode <- feature
extree <- Node$new(startNode)
TrainID3(extree, MyTrain)
plot(extree)
ig <- sapply(colnames(MyTrain)[-ncol(MyTrain)],
function(x) InformationGain(
table(MyTrain[,x], MyTrain[,ncol(MyTrain)])
)
)
feature <- names(ig)[ig == max(ig)][1]
startNode <- feature
extree <- Node$new(startNode)
TrainID3(extree, MyTrain)
print(tree, "feature", "obsCount")
Prune(tree, function(x) x$obsCount> 3000)
plot(extree)
ig <- sapply(colnames(MyTrain)[-ncol(MyTrain)],
function(x) InformationGain(
table(MyTrain[,x], MyTrain[,ncol(MyTrain)])
)
)
feature <- names(ig)[ig == max(ig)][1]
startNode <- feature
tree <- Node$new(startNode)
TrainID3(extree, MyTrain)
print(tree, "feature", "obsCount")
Prune(tree, function(x) x$obsCount> 3000)
plot(extree)
ig <- sapply(colnames(MyTrain)[-ncol(MyTrain)],
function(x) InformationGain(
table(MyTrain[,x], MyTrain[,ncol(MyTrain)])
)
)
feature <- names(ig)[ig == max(ig)][1]
startNode <- feature
tree <- Node$new(startNode)
TrainID3(extree, MyTrain)
print(tree, "feature", "obsCount")
Prune(tree, function(x) x$obsCount> 3000)
plot(tree)
ig <- sapply(colnames(MyTrain)[-ncol(MyTrain)],
function(x) InformationGain(
table(MyTrain[,x], MyTrain[,ncol(MyTrain)])
)
)
feature <- names(ig)[ig == max(ig)][1]
startNode <- feature
tree <- Node$new(startNode)
TrainID3(tree, MyTrain)
print(tree, "feature", "obsCount")
Prune(tree, function(x) x$obsCount> 3000)
plot(tree)
